{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "import os\n",
    "\n",
    "# Load the dataset (replace with your actual file path)\n",
    "filename = os.path.join(os.getcwd(), \"percent_return_threeYear.csv\")\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "filenameSector = os.path.join(os.getcwd(), \"sectorsCompany.csv\")\n",
    "dfSector = pd.read_csv(filenameSector)\n",
    "\n",
    "#convert original dataframe into rows as company names\n",
    "stacked_df = df.melt(id_vars=[\"date\"], var_name=\"Symbol\", value_name=\"Data\")\n",
    "merged_df = pd.merge(stacked_df, dfSector, on=\"Symbol\", how=\"inner\")\n",
    "\n",
    "sector_groups = merged_df.groupby(\"Sector\")\n",
    "sector_dfs = {sector: group.drop(columns=[\"Sector\"]) for sector, group in sector_groups} # dfs clustered by sector\n",
    "\n",
    "dfSectors = {}\n",
    "    \n",
    "for sector, df in sector_dfs.items() :\n",
    "    pivoted_df = df.pivot_table(index=\"date\", columns=\"Symbol\", values=\"Data\", aggfunc=\"first\").reset_index()\n",
    "    dfSectors[sector] = pivoted_df\n",
    "\n",
    "print(len(dfSectors)) # 11 clusters for each sector\n",
    "\n",
    "df_tech = dfSectors['Technology']\n",
    "df_basicMaterials = dfSectors['Basic Materials']\n",
    "df_communicationServices = dfSectors['Communication Services']\n",
    "df_consumerCyclical = dfSectors['Consumer Cyclical']\n",
    "df_consumerDefensive = dfSectors['Consumer Defensive']\n",
    "df_energy = dfSectors['Energy']\n",
    "df_healthcare = dfSectors['Healthcare']\n",
    "df_industrials = dfSectors['Industrials']\n",
    "df_real_estate = dfSectors['Real Estate']\n",
    "df_utilities = dfSectors['Utilities']\n",
    "df_financialServices = dfSectors['Financial Services']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1006, 493)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat next steps of code for EVERY Sector to form predictive model per cluster\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "# Iterate over the dataset in chunks of 5 rows (representing one week)\n",
    "for i in range(0, len(df_financialServices), 5): # replace df with specific df for every sector\n",
    "    if i + 4 >= len(df_financialServices):  # Prevent going out of bounds\n",
    "        break\n",
    "    # Monday to Thursday data (features)\n",
    "    X.extend(df_financialServices.iloc[i:i+4, 1:].T.values.tolist()) # rows are companies\n",
    "    #X += df.iloc[i:i+4, 1:].T.values\n",
    "    # Friday data (target)\n",
    "    y.extend(df_financialServices.iloc[i+4, 1:].T.values.tolist())  # Friday returns as target\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Train-test split (e.g., 80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -6.41951183 -11.87933395 -11.60719056 ...  -0.50351207   0.21964489\n",
      "   0.33913448]\n"
     ]
    }
   ],
   "source": [
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -3.90664276  -3.74174451  -5.52781224  -7.35206823]\n",
      " [ -7.85557429  -8.97170169 -10.67704956  -9.06064071]\n",
      " [ -9.23615876  -8.38814381 -11.90581004 -10.70725759]\n",
      " ...\n",
      " [ -0.07946497  -1.09601965  -1.36186914   1.21841673]\n",
      " [ -0.1800716   -0.69364491  -0.16630175  -0.65898605]\n",
      " [ -1.88296658   0.8016432    0.38276051  -0.05171342]]\n",
      "[-6.50953095 -3.49555009 -3.91276777 -7.53743451]\n",
      "[[ -6.50953095  -3.49555009  -3.91276777  -7.53743451]\n",
      " [ -0.38637407  -3.21955444  -3.23582131  -1.93537966]\n",
      " [ -0.56999523  -3.43180574  -2.09577462  -3.73654988]\n",
      " ...\n",
      " [ -8.07991369  -9.78880011  -7.77502001  -8.15644529]\n",
      " [ -8.71299205  -8.10887118 -10.20830255  -8.54714609]\n",
      " [ -9.9899424   -8.08064182 -11.47602296  -2.10588012]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10612,)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X) #many arrays of size 4\n",
    "print(X_train[0])\n",
    "print(X_train)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "# Train an XGBoost regression:squared error model - fine tuning parameters; importance to task\n",
    "model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=300, learning_rate=0.007, max_depth = 5)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Baseline model that predicts the mean y for all feature values\n",
    "baseline_model = DummyRegressor(strategy=\"mean\")\n",
    "baseline_model.fit(X_train, y_train)\n",
    "baseline_predictions = baseline_model.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model RMSE: 1.7064150024672238\n",
      "Baseline RMSE: 4.300053989616015\n",
      "Model Improvement: 2.5936389871487915\n",
      "Model R^2 Score: 0.8425011553466777\n",
      "Baseline R^2 Score: -0.00012864865095818523\n"
     ]
    }
   ],
   "source": [
    "### Model evaluation\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "print(f\"Model RMSE: {rmse}\")\n",
    "\n",
    "# Calculate RMSE for baseline\n",
    "baseline_rmse = np.sqrt(mean_squared_error(y_test, baseline_predictions))\n",
    "print(\"Baseline RMSE:\", baseline_rmse)\n",
    "print(\"Model Improvement:\", baseline_rmse - rmse)\n",
    "\n",
    "# Calculate R^2 scores\n",
    "model_r2 = r2_score(y_test, predictions)\n",
    "baseline_r2 = r2_score(y_test, baseline_predictions)\n",
    "\n",
    "print(f\"Model R^2 Score: {model_r2}\")\n",
    "print(f\"Baseline R^2 Score: {baseline_r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'subsample': 0.5, 'n_estimators': 250, 'min_child_weight': 10, 'max_depth': 7, 'max_delta_step': 8, 'learning_rate': 0.02, 'lambda': 4, 'gamma': 10}\n",
      "Best R^2 Score: 0.8608815421040038\n",
      "Standard Deviation of Predicted Values: 3.9319568\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n",
    "# Define the parameter distributions\n",
    "param_dist = {\n",
    "    'learning_rate': [0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.2],\n",
    "    'gamma': [0, 2, 4, 6, 8, 10],\n",
    "    'max_depth': [3, 4, 5, 6, 7],\n",
    "    'min_child_weight': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'max_delta_step': [0, 2, 4, 6, 8, 10],\n",
    "    'lambda': [0, 1, 2, 4, 6],\n",
    "    'n_estimators': [100, 150, 200, 250, 300],\n",
    "    'subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "}\n",
    "\n",
    "# Initialize the XGBoost model\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror')\n",
    "\n",
    "# Set up RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=100,  # Number of parameter settings to sample\n",
    "    scoring='r2',  # Optimize for R^2 score\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    random_state=42,  # For reproducibility\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "# Fit the random search\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Output best parameters and best score\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best R^2 Score:\", random_search.best_score_)\n",
    "\n",
    "# Retrieve the best model\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate the standard deviation of the predicted values\n",
    "std_predicted_values = np.std(y_pred)\n",
    "print(\"Standard Deviation of Predicted Values:\", std_predicted_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
