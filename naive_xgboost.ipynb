{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           date             A          AAPL          ABBV          ABNB  \\\n",
      "0    2023-09-01  6.955830e+05  2.428850e+07  2.371953e+06  3.329691e+06   \n",
      "1    2023-09-05  1.079488e+06  2.401770e+07  2.802535e+06  1.479560e+07   \n",
      "2    2023-09-06  1.427409e+06  4.497533e+07  3.813672e+06  5.018169e+06   \n",
      "3    2023-09-07  1.142687e+06  6.374670e+07  3.615118e+06  5.533647e+06   \n",
      "4    2023-09-08  1.583872e+06  3.701821e+07  3.048799e+06  6.395867e+06   \n",
      "..          ...           ...           ...           ...           ...   \n",
      "246  2024-08-26  7.222382e+05  1.348518e+07  2.135079e+06  4.276281e+06   \n",
      "247  2024-08-27  6.740402e+05  1.577595e+07  1.288436e+06  2.961524e+06   \n",
      "248  2024-08-28  7.737799e+05  1.681921e+07  2.017006e+06  2.946977e+06   \n",
      "249  2024-08-29  1.108376e+06  2.261333e+07  2.343655e+06  2.856675e+06   \n",
      "250  2024-08-30  1.100656e+06  2.316545e+07  1.961752e+06  2.795314e+06   \n",
      "\n",
      "              ABT          ACGL            ACN           ADBE           ADI  \\\n",
      "0    2.633559e+06  1.201888e+06  539307.985316  396253.032246  1.047855e+06   \n",
      "1    4.355253e+06  1.781555e+06  543141.011330  415864.449032  1.394308e+06   \n",
      "2    5.144473e+06  1.269459e+06  390099.019578  409178.568888  1.888974e+06   \n",
      "3    3.935638e+06  2.993561e+06  476470.288489  372700.898920  3.171056e+06   \n",
      "4    3.585181e+06  1.810387e+06  444041.202717  403569.080436  1.824276e+06   \n",
      "..            ...           ...            ...            ...           ...   \n",
      "246  2.864001e+06  1.179378e+06  504409.739447  247915.871934  1.126738e+06   \n",
      "247  2.403127e+06  1.374496e+06  635327.442600  247231.897764  1.605353e+06   \n",
      "248  3.414233e+06  9.382356e+05  454631.754650  229981.715944  1.419292e+06   \n",
      "249  3.007802e+06  1.250286e+06  475830.744727  343210.567076  1.178888e+06   \n",
      "250  3.831859e+06  2.547300e+06  588336.030917  345123.810991  1.184450e+06   \n",
      "\n",
      "     ...            WTW            WY          WYNN           XEL  \\\n",
      "0    ...  218930.858398  1.124788e+07  1.941746e+06  5.281488e+06   \n",
      "1    ...  193204.187746  1.333643e+07  3.210692e+06  7.861585e+06   \n",
      "2    ...  144170.158331  1.062794e+07  2.006152e+06  7.096622e+06   \n",
      "3    ...  221991.844073  1.015383e+07  3.339518e+06  6.581713e+06   \n",
      "4    ...  190832.168894  1.059787e+07  1.570076e+06  6.661370e+06   \n",
      "..   ...            ...           ...           ...           ...   \n",
      "246  ...  140687.217711  8.775288e+06  1.707483e+06  6.037038e+06   \n",
      "247  ...  137725.373666  9.724473e+06  1.477773e+06  4.382374e+06   \n",
      "248  ...  119962.088216  1.203051e+07  4.011588e+06  2.569069e+06   \n",
      "249  ...  124065.120163  9.718950e+06  2.219780e+06  4.830714e+06   \n",
      "250  ...  290767.035645  1.731082e+07  5.137899e+06  4.708423e+06   \n",
      "\n",
      "              XOM           XYL           YUM           ZBH           ZBRA  \\\n",
      "0    1.363137e+07  1.183975e+06  8.604701e+05  1.237838e+06  112927.717414   \n",
      "1    1.682494e+07  2.773126e+06  1.137845e+06  1.410928e+06  152617.291857   \n",
      "2    1.214387e+07  1.566033e+06  1.374454e+06  2.986282e+06  120026.291994   \n",
      "3    1.213303e+07  1.909937e+06  1.119826e+06  1.523041e+06  188402.883140   \n",
      "4    1.289280e+07  1.796021e+06  5.873655e+05  9.250032e+05  169576.642464   \n",
      "..            ...           ...           ...           ...            ...   \n",
      "246  1.160106e+07  5.948715e+05  1.286391e+06  7.985831e+05   71285.429048   \n",
      "247  8.560150e+06  1.080670e+06  1.545699e+06  7.990037e+05   68331.393525   \n",
      "248  9.295451e+06  6.815772e+05  1.607051e+06  6.028661e+05   61325.706962   \n",
      "249  9.178150e+06  5.254211e+05  1.155538e+06  7.004158e+05   93159.710242   \n",
      "250  1.135640e+07  9.183178e+05  1.402880e+06  1.375634e+06  131204.648094   \n",
      "\n",
      "              ZTS  \n",
      "0    6.173786e+05  \n",
      "1    8.892636e+05  \n",
      "2    7.349522e+05  \n",
      "3    6.250404e+05  \n",
      "4    8.511485e+05  \n",
      "..            ...  \n",
      "246  5.607655e+05  \n",
      "247  7.836859e+05  \n",
      "248  8.139177e+05  \n",
      "249  7.452237e+05  \n",
      "250  1.271914e+06  \n",
      "\n",
      "[251 rows x 498 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "\n",
    "# Load the dataset (replace with your actual file path)\n",
    "#df = pd.read_csv('data/2024-06-01_2024-09-01/aggregated/percent_return.csv')\n",
    "#df = pd.read_csv('data/2024-08-20_2024-08-28/aggregated/percent_return.csv')\n",
    "df = pd.read_csv('data/2023-09-01_2024-09-01/aggregated/percent_return.csv') # 1 year's worth of data\n",
    "\n",
    "# Extract stock names\n",
    "stock_names = df.columns[1:]  # Assuming the first column is the date\n",
    "\n",
    "print(df) #every 5th row is a friday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for Monday to Thursday (features) and Friday (target)\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "# Iterate over the dataset in chunks of 5 rows (representing one week)\n",
    "for i in range(0, len(df), 5):\n",
    "    if i + 4 >= len(df):  # Prevent going out of bounds\n",
    "        break\n",
    "    # Monday to Thursday data (features)\n",
    "    X.extend(df.iloc[i:i+4, 1:].T.values.tolist()) # rows are companies\n",
    "    #X += df.iloc[i:i+4, 1:].T.values\n",
    "    # Friday data (target)\n",
    "    y.extend(df.iloc[i+4, 1:].T.values.tolist())  # Friday returns as target\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Train-test split (e.g., 80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'subsample': 1.0, 'n_estimators': 300, 'min_child_weight': 6, 'max_depth': 6, 'max_delta_step': 0, 'learning_rate': 0.01, 'lambda': 6, 'gamma': 6}\n",
      "Best R^2 Score: 0.8236557409913845\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n",
    "# Define the parameter distributions\n",
    "param_dist = {\n",
    "    'learning_rate': [0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.2],\n",
    "    'gamma': [0, 2, 4, 6, 8, 10],\n",
    "    'max_depth': [3, 4, 5, 6, 7],\n",
    "    'min_child_weight': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'max_delta_step': [0, 2, 4, 6, 8, 10],\n",
    "    'lambda': [0, 1, 2, 4, 6],\n",
    "    'n_estimators': [100, 150, 200, 250, 300],\n",
    "    'subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "}\n",
    "\n",
    "# Initialize the XGBoost model\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror')\n",
    "\n",
    "# Set up RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=100,  # Number of parameter settings to sample\n",
    "    scoring='r2',  # Optimize for R^2 score\n",
    "    cv=TimeSeriesSplit(n_splits=5),  # 5-fold cross-validation, with time series(stock data)\n",
    "    random_state=42,  # For reproducibility\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "# Fit the random search\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Output best parameters and best score\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best R^2 Score:\", random_search.best_score_)\n",
    "\n",
    "# Retrieve the best model\n",
    "best_model = random_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "# Train an XGBoost regression:squared error model - fine tuning parameters; importance to task\n",
    "model = xgb.XGBRegressor(objective='reg:squarederror', subsample = 0.6, n_estimators=300, min_child_weight = 4, max_depth = 4, max_delta_step = 0, learning_rate=0.02, gamma = 10)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Baseline model that predicts the mean y for all feature values\n",
    "baseline_model = DummyRegressor(strategy=\"mean\")\n",
    "baseline_model.fit(X_train, y_train)\n",
    "baseline_predictions = baseline_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "predictions = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model RMSE: 33582281.964153245\n",
      "Baseline RMSE: 60333984.856081285\n",
      "Model Improvement: 26751702.89192804\n",
      "Model R^2 Score: 0.6901470130816633\n",
      "Baseline R^2 Score: -0.00013655132404921666\n"
     ]
    }
   ],
   "source": [
    "### Model evaluation\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "print(f\"Model RMSE: {rmse}\")\n",
    "\n",
    "# Calculate RMSE for baseline\n",
    "baseline_rmse = np.sqrt(mean_squared_error(y_test, baseline_predictions))\n",
    "print(\"Baseline RMSE:\", baseline_rmse)\n",
    "print(\"Model Improvement:\", baseline_rmse - rmse)\n",
    "\n",
    "# Calculate R^2 scores\n",
    "model_r2 = r2_score(y_test, predictions)\n",
    "baseline_r2 = r2_score(y_test, baseline_predictions)\n",
    "\n",
    "print(f\"Model R^2 Score: {model_r2}\")\n",
    "print(f\"Baseline R^2 Score: {baseline_r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model RMSE: 33582281.964153245\n",
      "Baseline RMSE: 60333984.856081285\n",
      "Model Improvement: 26751702.89192804\n",
      "Model R^2 Score: 0.6901470130816633\n",
      "Baseline R^2 Score: -0.00013655132404921666\n"
     ]
    }
   ],
   "source": [
    "### Model evaluation\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "print(f\"Model RMSE: {rmse}\")\n",
    "\n",
    "# Calculate RMSE for baseline\n",
    "baseline_rmse = np.sqrt(mean_squared_error(y_test, baseline_predictions))\n",
    "print(\"Baseline RMSE:\", baseline_rmse)\n",
    "print(\"Model Improvement:\", baseline_rmse - rmse)\n",
    "\n",
    "# Calculate R^2 scores\n",
    "model_r2 = r2_score(y_test, predictions)\n",
    "baseline_r2 = r2_score(y_test, baseline_predictions)\n",
    "\n",
    "print(f\"Model R^2 Score: {model_r2}\")\n",
    "print(f\"Baseline R^2 Score: {baseline_r2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
